{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netflix appetency - Consumer Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://i.pcmag.com/imagery/reviews/05cItXL96l4LE9n02WfDR0h-5.fit_scale.size_1028x578.v1582751026.png)\n",
    "\n",
    "\n",
    "## Goal:\n",
    "\n",
    "- Classify consumers according to their appetite to subscribe to Netflix.\n",
    "\n",
    "## Metric:\n",
    "\n",
    "- The metric used is AUC\n",
    "\n",
    "## Data:\n",
    "\n",
    "- train.csv - the training set. it consists of an id column, the customers features, and a target column: target.\n",
    "- test.csv - the test set. it consists of everything in train.csv except target.\n",
    "\n",
    "For reasons of confidentiality, the data is anonymized and augmented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install catboost --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('D:/D_ST/GitProfile/NetflixAppentency/Data/train.csv.gz', compression= 'gzip')\n",
    "test_df = pd.read_csv('D:/D_ST/GitProfile/NetflixAppentency/Data/test.csv.gz', compression= 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 509)\n",
      "(30000, 508)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_497</th>\n",
       "      <th>feature_498</th>\n",
       "      <th>feature_499</th>\n",
       "      <th>feature_500</th>\n",
       "      <th>feature_501</th>\n",
       "      <th>feature_502</th>\n",
       "      <th>feature_503</th>\n",
       "      <th>feature_504</th>\n",
       "      <th>feature_505</th>\n",
       "      <th>feature_506</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C0</td>\n",
       "      <td>C0</td>\n",
       "      <td>C1</td>\n",
       "      <td>C5</td>\n",
       "      <td>C11</td>\n",
       "      <td>37.56</td>\n",
       "      <td>54.756667</td>\n",
       "      <td>54.756667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C0</td>\n",
       "      <td>C0</td>\n",
       "      <td>C3</td>\n",
       "      <td>C5</td>\n",
       "      <td>C1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>C0</td>\n",
       "      <td>C0</td>\n",
       "      <td>C3</td>\n",
       "      <td>C5</td>\n",
       "      <td>C2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>C0</td>\n",
       "      <td>C0</td>\n",
       "      <td>C1</td>\n",
       "      <td>C5</td>\n",
       "      <td>C1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>C0</td>\n",
       "      <td>C0</td>\n",
       "      <td>C3</td>\n",
       "      <td>C3</td>\n",
       "      <td>C11</td>\n",
       "      <td>37.48</td>\n",
       "      <td>37.480000</td>\n",
       "      <td>37.161333</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 509 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target feature_0 feature_1 feature_2 feature_3 feature_4  feature_5  \\\n",
       "0   0       0        C0        C0        C1        C5       C11      37.56   \n",
       "1   1       0        C0        C0        C3        C5        C1        NaN   \n",
       "2   2       0        C0        C0        C3        C5        C2        NaN   \n",
       "3   3       0        C0        C0        C1        C5        C1        NaN   \n",
       "4   4       1        C0        C0        C3        C3       C11      37.48   \n",
       "\n",
       "   feature_6  feature_7  ...  feature_497  feature_498  feature_499  \\\n",
       "0  54.756667  54.756667  ...            0            0            0   \n",
       "1        NaN        NaN  ...            0            0            0   \n",
       "2        NaN        NaN  ...            0            0            0   \n",
       "3        NaN        NaN  ...            0            0            0   \n",
       "4  37.480000  37.161333  ...            0            0            0   \n",
       "\n",
       "   feature_500  feature_501  feature_502  feature_503  feature_504  \\\n",
       "0            0            0            0            0            0   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            0            0            0            0   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "  feature_505 feature_506  \n",
       "0           0           0  \n",
       "1           0           0  \n",
       "2           0           0  \n",
       "3           0           0  \n",
       "4           0           0  \n",
       "\n",
       "[5 rows x 509 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature_500, Feature_502, Feature_505 have min max mean everything as 0.\n",
    "- Feature_497, Feature_499, Feature_504 oddly sahre same min, mean and Standard deviation.\n",
    "- Some columns has only one categorical variable.\n",
    "- Need to take care of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handeling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Function to calculate % of missing vazlues in each column\n",
    "def find_missings_percnt(df):\n",
    "    if df.isna().sum().sum()>0:\n",
    "        ColumnName , PercentMissing = list(), list()\n",
    "        for column in df.columns:\n",
    "            if df[column].isnull().sum():\n",
    "                ColumnName.append(column)\n",
    "                PercentMissing.append((df[column].isnull().sum()/len(df[column])*100))\n",
    "        missing_percent_df = pd.DataFrame({'Feature':ColumnName, 'PercentMissing':PercentMissing})\n",
    "        return missing_percent_df.sort_values(by = 'PercentMissing', ascending = False)\n",
    "    else:\n",
    "        txt = \"No Feature has missing Values\" \n",
    "        return print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding missing value for train data\n",
    "train_Percent_Missings_df=find_missings_percnt(train_df)\n",
    "train_Percent_Missings_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding missing value for test data\n",
    "test_Percent_Missings_df=find_missings_percnt(test_df)\n",
    "test_Percent_Missings_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Distribution of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting missing value for train data\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(x=train_Percent_Missings_df.PercentMissing).set_title('Missing Values Distribution in train data',size=20)\n",
    "plt.ylabel('The number of features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting missing value for test data\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(x=test_Percent_Missings_df.PercentMissing).set_title('Missing Values Distribution in test data',size=20)\n",
    "plt.ylabel('The number of features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dropping Featurees with more tha 25% missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get columns with more than 25% missing values\n",
    "Missings_more_than_25_train = train_Percent_Missings_df[train_Percent_Missings_df.PercentMissing>25]\n",
    "Missings_more_than_25_test = test_Percent_Missings_df[test_Percent_Missings_df.PercentMissing>25]\n",
    "\n",
    "#Drop from train and test \n",
    "train_df = train_df.drop(list(Missings_more_than_25_train.Feature), axis=1)\n",
    "test_df=test_df.drop(list(Missings_more_than_25_test.Feature),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Numerical vs Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numeric\n",
    "Num_Features = train_df.columns[train_df.dtypes !='object'].values[2:]\n",
    "\n",
    "#Categorical\n",
    "Cat_Features = train_df.columns[train_df.dtypes =='object'].values\n",
    "print(len(Num_Features))\n",
    "print(len(Cat_Features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the distribution using Pie chart\n",
    "pichrt = [len(Cat_Features),len(Num_Features)]\n",
    "colors = sns.color_palette('coolwarm')\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('The percentage of numeric and categorical features',size=15)\n",
    "plt.pie(pichrt, colors = colors, labels=['Categorical','Numeric'] ,autopct = '%0.0f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing Categorical Variables with Mode\n",
    "def get_mode(feature,df):\n",
    "    return df[feature].mode()[0]\n",
    "\n",
    "for col in Cat_Features:\n",
    "    train_df[col].fillna(get_mode(col,train_df),inplace=True)\n",
    "    test_df[col].fillna(get_mode(col,test_df),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing Numerical variables with median\n",
    "for col in Num_Features:\n",
    "    train_df[col].fillna(train_df[col].median(),inplace=True)\n",
    "    test_df[col].fillna(test_df[col].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handelling Date Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns that contain date objects\n",
    "mask = train_df[Cat_Features][:10].astype(str).apply(lambda x : x.str.match('(\\d{2,4}(-|\\/|\\\\|\\.| )\\d{2}(-|\\/|\\\\|\\.| )\\d{2,4})+').any())\n",
    "date_Features=train_df[Cat_Features].loc[:,mask]\n",
    "\n",
    "date_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these columns to Datetime\n",
    "train_df[date_Features.columns] = train_df[date_Features.columns].apply(pd.to_datetime,dayfirst = False)\n",
    "test_df[date_Features.columns] = test_df[date_Features.columns].apply(pd.to_datetime,dayfirst = False)\n",
    "\n",
    "#Show date columns\n",
    "shw_dateCol = train_df[train_df.columns[train_df.dtypes =='datetime64[ns]']]\n",
    "shw_dateCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Year from date Using DatetimeIndex\n",
    "for col in shw_dateCol.columns:\n",
    "    train_df[col+\"_year\"]=pd.DatetimeIndex(train_df[col]).year\n",
    "    test_df[col+\"_year\"]=pd.DatetimeIndex(test_df[col]).year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Date Features since we will only consider the Year\n",
    "train_df.drop(list(shw_dateCol.columns),axis=1,inplace=True)\n",
    "test_df.drop(list(shw_dateCol.columns),axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update Categorical_Features list\n",
    "Cat_Features = [i for i in Cat_Features if i not in shw_dateCol.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handeling Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Copy of data\n",
    "train_c = train_df.copy()\n",
    "test_c = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values for each Categorical Feature\n",
    "def get_cardinality(df,otype):\n",
    "    tmp=[]\n",
    "    for col in otype:\n",
    "        tmp.append( df[col].nunique() )\n",
    "\n",
    "    cardinality = pd.DataFrame({'Features':otype,'Cardinality':tmp})\n",
    "    return cardinality.sort_values(by='Cardinality',ascending=False)\n",
    "\n",
    "# Categorical Features Sorted by cardinality \n",
    "cat_cardinality = get_cardinality(train_c,Cat_Features).head(10)\n",
    "cat_cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for values that has only one count\n",
    "print(train_c['feature_18'].value_counts()[train_c['feature_18'].value_counts()<2].count())\n",
    "print(train_c['feature_133'].value_counts()[train_c['feature_133'].value_counts()<2].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 7181 unique values out of 15858 that come up only once in the entire dataset for 'feature_18'.\n",
    "\n",
    "There are 4916 unique values out of 7126 that come up only once in the entire dataset for 'feature_133'.\n",
    "\n",
    "We need to reduce the cardinality of the Categorical variables. We will replace the value which comes only once as \"other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing value which comes only once to \"other\"\n",
    "for col in list(cat_cardinality.Features):\n",
    "    tmp = list(train_c[col].value_counts()[train_c[col].value_counts()<2].index)\n",
    "    train_c[col] = train_c[col].apply(lambda i:'other' if i in tmp else i)\n",
    "\n",
    "# Repeating the same step for test data\n",
    "for col in list(cat_cardinality.Features):\n",
    "    tmp = list(test_c[col].value_counts()[test_c[col].value_counts()<2].index)\n",
    "    test_c[col] = test_c[col].apply(lambda i:'other' if i in tmp else i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Chi-Square Hypotheis Test for Indepence for Categorical Features\n",
    "\n",
    "- p-value = 0.05\n",
    "\n",
    "- Null Hypothesis (Ho): The 'Target' Variable is independent on the Cat_Features\n",
    "\n",
    "- Alternate Hypothesis (Ha): The 'Target' Variable is dependent on the Cat_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing chi-square\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2_result = []\n",
    "for i in Cat_Features:\n",
    "    if chi2_contingency(pd.crosstab(train_df['target'],train_df[i]))[1] < 0.05:\n",
    "        chi2_result.append('Reject Null Hypothesis')\n",
    "    else:\n",
    "        chi2_result.append('Fail to Reject Null Hypothesis')\n",
    "        \n",
    "result = pd.DataFrame({'Column':Cat_Features, 'HypotheisisResult': chi2_result})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the features to drop list which is \"Fail to Reject Null Hypothesis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the features to drop list - Unwanted_Cat_Features\n",
    "Unwanted_Cat_Features = result[result.HypotheisisResult == 'Fail to Reject Null Hypothesis'].Column.values\n",
    "\n",
    "#Creating a drop list\n",
    "Features_toDrop=[]\n",
    "Features_toDrop += list(Unwanted_Cat_Features)\n",
    "print('Number of Categorical variables that are not dependent on variable target = ',len(Unwanted_Cat_Features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handeling Numeric Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations between numeric columns and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df=train_c.drop(['target'],axis=1).corrwith(train_c['target']).abs().sort_values(ascending=False)\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numeric features in terms of correlation\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel('Correlation with target')\n",
    "plt.ylabel('The number of numeric features')\n",
    "sns.histplot(corr_df).set_title('Distribution of numeric features in terms of correlation with the target',size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot features which have more than 0.08 correlation with the target (sorted)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(x=corr_df[corr_df>0.08],y=corr_df[corr_df>0.08].index).set_title('Correlation between numeric features and target')\n",
    "plt.xlabel('Correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_307 is the only feature with correlation coefficient value greater than 0.3 and it\n",
    "is binary just like the 'target' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing top 12 features \n",
    "corr_df[corr_df>0.08].head(12).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting top 10 features \n",
    "fig, ax = plt.subplots(3,3,figsize=(17,12))\n",
    "sns.histplot(train_c.feature_81, ax=ax[0][0])\n",
    "sns.histplot(train_c.feature_82, ax=ax[0][1])\n",
    "sns.countplot(train_c.feature_307,hue=train_c.target, ax=ax[0][2])\n",
    "sns.countplot(train_c.feature_297,hue=train_c.target, ax=ax[1][0])\n",
    "sns.countplot(train_c.feature_108,hue=train_c.target, ax=ax[1][1])\n",
    "sns.countplot(train_c.feature_110,hue=train_c.target, ax=ax[1][2])\n",
    "sns.countplot(train_c.feature_470,hue=train_c.target, ax=ax[2][0])\n",
    "sns.countplot(train_c.feature_264,hue=train_c.target, ax=ax[2][1])\n",
    "sns.countplot(train_c.feature_263,hue=train_c.target, ax=ax[2][2])\n",
    "plt.suptitle('Important numeric features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding New Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binarization feature_82\n",
    "train_c['feature_82_binary']= train_c['feature_82'].apply(lambda x: 0 if x==0 else 1)\n",
    "test_c['feature_82_binary']= test_c['feature_82'].apply(lambda x: 0 if x==0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking sqrt of feature_81\n",
    "train_c['feature_81_sqrt']= train_c.feature_81.apply(np.sqrt)\n",
    "test_c['feature_81_sqrt']= test_c.feature_81.apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiplication of feature_307 and feature_82_binary\n",
    "train_c['feature_82_307']= train_c['feature_307']*train_c['feature_82_binary']\n",
    "test_c['feature_82_307']= test_c['feature_307']*test_c['feature_82_binary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding features to drop list that have insignificant correlation with 'target' variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28  features added to drop list\n"
     ]
    }
   ],
   "source": [
    "#Corr less than 0.002 with target\n",
    "corr_low=list(corr_df[corr_df<0.002].index)\n",
    "\n",
    "#Add to drop list\n",
    "Features_toDrop+=corr_low\n",
    "print(len(corr_low),' features added to drop list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Features with one value to drop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59  features added to drop list\n"
     ]
    }
   ],
   "source": [
    "# Find columns with one value\n",
    "Col_with_OneValue=[]\n",
    "for col in train_c.columns:\n",
    "    if (( train_c[col].nunique() <=1 )):\n",
    "        Col_with_OneValue.append(col)\n",
    "train_c[Col_with_OneValue]\n",
    "\n",
    "#Add to drop list\n",
    "Features_toDrop +=list(Col_with_OneValue)\n",
    "print(len(Col_with_OneValue),' features added to drop list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Correlation between num columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_corr_df = train_c[Num_Features].corr()\n",
    "get_corr_df = get_corr_df[get_corr_df > 0.9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_corr_df.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are columns with corr >0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting features that have corr >= 0.9\n",
    "corr_features = np.where(np.abs(get_corr_df) > 0.9)\n",
    "\n",
    "# To remove self corr of feature and avoiding duplication\n",
    "corr_features = [(get_corr_df.iloc[x,y], x, y) for x, y in zip(*corr_features) if x != y and x < y]\n",
    "\n",
    "# Looping through array to create df\n",
    "p,q,r = list(),list(),list()\n",
    "for x,y,z in corr_features:\n",
    "    p.append(get_corr_df.index[y])\n",
    "    q.append(get_corr_df.columns[z])\n",
    "    r.append(x)\n",
    "corr_arr = pd.DataFrame([p,q,r], index=['C1', 'C2', 'Corr']).T\n",
    "corr_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop one column from the pair to avoid multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188  features added to drop list\n"
     ]
    }
   ],
   "source": [
    "high_corr_mul_drop = list( set(corr_arr.C1[corr_arr.Corr > 0.9])) +  list(set(corr_arr.C1[corr_arr.Corr <-0.9]))\n",
    "Features_toDrop += high_corr_mul_drop\n",
    "print(len(high_corr_mul_drop),' features added to drop list')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping the unnecessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_toDrop=list(set(Features_toDrop))\n",
    "\n",
    "train_c.drop(Features_toDrop,axis=1,inplace=True)\n",
    "test_c.drop(Features_toDrop,axis=1,inplace=True)\n",
    "\n",
    "print(len(Features_toDrop),' Features dropped')\n",
    "print(train_c.shape) \n",
    "print(test_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Cat_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create copy\n",
    "train_m = train_c.copy()\n",
    "test_m = test_c.copy()\n",
    "\n",
    "#Remove dropped Columns from Cat_Features list\n",
    "Cat_Features = [i for i in Cat_Features\n",
    "                        if i not in Features_toDrop]\n",
    "\n",
    "# Encoding categorical features\n",
    "for i in list(Cat_Features):\n",
    "    Lab_En = LabelEncoder()\n",
    "    Lab_En.fit(pd.concat([train_m[i],test_m[i]], ignore_index=True))\n",
    "\n",
    "    # Transform \n",
    "    train_m[i] = Lab_En.transform(train_m[i])\n",
    "    test_m[i] = Lab_En.transform(test_m[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- What is sns.set()\n",
    "\n",
    "- Diff b/w pip install and import\n",
    "\n",
    "- How to plot prallel hist\n",
    "\n",
    "- Try keeping month in analysis\n",
    "\n",
    "- Replacing value which comes only once to \"other\" What does .Index do ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "\n",
    "- The Confusionj Matrix: https://www.youtube.com/watch?v=Kdsp6soqA7o\n",
    "- ROC and AUC: https://www.youtube.com/watch?v=4jRBRDbJemM\n",
    "- Chi Square: https://www.jmp.com/en_be/statistics-knowledge-portal/chi-square-test.html\n",
    "- Chi Square: https://www.analyticsvidhya.com/blog/2019/11/what-is-chi-square-test-how-it-works/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "254e17d4be08a24f978ade82290d93b3107a00fee441fe8b2dcb1436d2542d70"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
